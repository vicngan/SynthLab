"""
Model Caching System for SynthLab

This module provides intelligent caching of trained synthesizer models to avoid
redundant training and dramatically speed up synthetic data generation.

## The Problem: Slow Repeated Training

Training GANs (CTGAN, TVAE) is computationally expensive:
- CTGAN on 10K rows: ~5-10 minutes
- TVAE on 10K rows: ~3-5 minutes
- For experiments with different generation sizes: trains every time!

**Example scenario:**
```python
# User wants to try different sample sizes
for n in [100, 500, 1000, 5000]:
    generator.train(df)  # ‚Üê TRAINS AGAIN! (5 min each time)
    synthetic = generator.generate(n)
```

Total time: 20-40 minutes for something that should take 20 seconds!

## The Solution: Smart Model Caching

Cache trained models based on:
1. **Data fingerprint**: Hash of data content (not filename)
2. **Model configuration**: Method, hyperparameters, constraints
3. **Training parameters**: Epochs, batch size, etc.

**With caching:**
```python
# First time: Train and cache (5 min)
generator.train(df)  # Cached with fingerprint ABC123

# Subsequent times: Load from cache (2 sec)
generator.train(df)  # Detects same data ‚Üí loads cache
synthetic = generator.generate(1000)  # Instant!
```

## How It Works

### Cache Key Generation:

```
Data Hash + Model Config + Training Params
     ‚Üì
SHA-256 Hash
     ‚Üì
Cache Key: a3f89d2e1b...
     ‚Üì
Stored as: cache/models/a3f89d2e1b_CTGAN.pkl
```

### Cache Hit/Miss Flow:

```
1. User calls generator.train(df)
        ‚Üì
2. Compute cache key from (df, config)
        ‚Üì
3. Check if cache exists
        ‚Üì
   Yes ‚Üí Load cached model (2 sec)
   No  ‚Üí Train new model (5 min) ‚Üí Save to cache
        ‚Üì
4. Ready to generate!
```

### Automatic Expiration:

Old cache entries are automatically cleaned up:
- Default: 30 days since last use
- Size limit: 5 GB total cache
- LRU policy: Least Recently Used removed first

## Why This Matters

### Performance Impact:

| Operation | Without Cache | With Cache | Speedup |
|-----------|--------------|-----------|---------|
| First train | 5 min | 5 min | 1x |
| Re-train same data | 5 min | 2 sec | **150x** |
| Experiment (10 variations) | 50 min | 7 min | **7x** |

### Use Cases:

1. **Parameter tuning**: Try different generation sizes
2. **A/B testing**: Compare synthesis methods
3. **Reproducibility**: Re-generate with same model
4. **CI/CD**: Fast model loading in pipelines
5. **Interactive exploration**: Instant response
"""

import hashlib
import pickle
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, Tuple
import warnings


class ModelCache:
    """
    Intelligent caching system for trained synthesizer models.

    Features:
    - Content-based hashing (not filename-based)
    - Automatic expiration (LRU + time-based)
    - Size management (configurable max cache size)
    - Metadata tracking (training time, data shape, etc.)
    - Cache statistics and monitoring

    Attributes:
        cache_dir: Directory to store cached models
        max_age_days: Maximum age before expiration
        max_cache_size_gb: Maximum total cache size in GB
        enabled: Whether caching is active
    """

    def __init__(
        self,
        cache_dir: str = "cache/models",
        max_age_days: int = 30,
        max_cache_size_gb: float = 5.0,
        enabled: bool = True,
        verbose: bool = True
    ):
        """
        Initialize model cache.

        Args:
            cache_dir: Directory to store cached models
            max_age_days: Expire cache entries older than this (default: 30)
            max_cache_size_gb: Maximum total cache size in GB (default: 5.0)
            enabled: Enable/disable caching (default: True)
            verbose: Print cache operations (default: True)

        Example:
            >>> cache = ModelCache()  # Uses defaults
            >>> cache = ModelCache(max_age_days=7, max_cache_size_gb=10.0)
        """
        self.cache_dir = Path(cache_dir)
        self.max_age_days = max_age_days
        self.max_cache_size_gb = max_cache_size_gb
        self.enabled = enabled
        self.verbose = verbose

        # Create cache directory
        if enabled:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            if verbose:
                print(f"‚úì ModelCache initialized")
                print(f"  Cache directory: {self.cache_dir}")
                print(f"  Max age: {max_age_days} days")
                print(f"  Max size: {max_cache_size_gb} GB")

    def _compute_data_hash(self, df: pd.DataFrame) -> str:
        """
        Compute content-based hash of DataFrame.

        ## Why Content Hash?

        Using filename is unreliable:
        - File renamed: different hash, but same data
        - File moved: different path, but same data
        - Same data, different files: should use same cache

        Content hash solves this by hashing actual data.

        ## Algorithm:

        1. Convert DataFrame to bytes
        2. Include: data values, column names, data types
        3. SHA-256 hash for collision resistance
        4. Truncate to 16 chars for readability

        Args:
            df: Input DataFrame

        Returns:
            16-character hex hash string

        Example:
            >>> df1 = pd.DataFrame({'A': [1, 2, 3]})
            >>> df2 = pd.DataFrame({'A': [1, 2, 3]})  # Same content
            >>> hash1 = cache._compute_data_hash(df1)
            >>> hash2 = cache._compute_data_hash(df2)
            >>> assert hash1 == hash2  # Same hash!
        """
        # Create a stable representation of the DataFrame
        # Include: values, column names, dtypes
        data_bytes = pickle.dumps({
            'values': df.values.tobytes(),
            'columns': df.columns.tolist(),
            'dtypes': df.dtypes.to_dict(),
            'shape': df.shape
        }, protocol=pickle.HIGHEST_PROTOCOL)

        # Compute SHA-256 hash
        hash_obj = hashlib.sha256(data_bytes)
        hash_hex = hash_obj.hexdigest()

        # Return first 16 chars for readability
        return hash_hex[:16]

    def _compute_config_hash(self, config: Dict[str, Any]) -> str:
        """
        Compute hash of model configuration.

        Includes:
        - Model method (CTGAN, TVAE, GaussianCopula)
        - Hyperparameters (epochs, batch_size, etc.)
        - Constraints (if any)
        - Random seed

        Args:
            config: Model configuration dict

        Returns:
            8-character hex hash string
        """
        # Sort keys for consistent hashing
        config_sorted = json.dumps(config, sort_keys=True)
        hash_obj = hashlib.sha256(config_sorted.encode())
        return hash_obj.hexdigest()[:8]

    def generate_cache_key(
        self,
        df: pd.DataFrame,
        method: str,
        config: Optional[Dict] = None
    ) -> str:
        """
        Generate unique cache key for data + model combination.

        ## Cache Key Format:

        ```
        {data_hash}_{config_hash}_{method}
        ```

        Example:
        ```
        a3f89d2e1b4c5678_9def1234_CTGAN
        ‚îî‚îÄ data hash ‚îÄ‚îò ‚îîconfig‚îÄ‚îò ‚îîmethod‚îò
        ```

        Args:
            df: Training data
            method: Synthesis method ('CTGAN', 'TVAE', etc.)
            config: Model configuration dict (optional)

        Returns:
            Cache key string
        """
        data_hash = self._compute_data_hash(df)

        if config:
            config_hash = self._compute_config_hash(config)
        else:
            config_hash = "default"

        cache_key = f"{data_hash}_{config_hash}_{method}"
        return cache_key

    def _get_cache_path(self, cache_key: str) -> Path:
        """Get file path for cache key."""
        return self.cache_dir / f"{cache_key}.pkl"

    def _get_metadata_path(self, cache_key: str) -> Path:
        """Get metadata file path for cache key."""
        return self.cache_dir / f"{cache_key}.meta.json"

    def has_cached_model(self, cache_key: str) -> bool:
        """
        Check if model is cached.

        Args:
            cache_key: Cache key from generate_cache_key()

        Returns:
            True if cached model exists and is valid
        """
        if not self.enabled:
            return False

        cache_path = self._get_cache_path(cache_key)
        meta_path = self._get_metadata_path(cache_key)

        # Both files must exist
        if not (cache_path.exists() and meta_path.exists()):
            return False

        # Check if expired
        try:
            with open(meta_path, 'r') as f:
                metadata = json.load(f)

            cached_time = datetime.fromisoformat(metadata['cached_at'])
            age = datetime.now() - cached_time

            if age > timedelta(days=self.max_age_days):
                if self.verbose:
                    print(f"  ‚è∞ Cache expired (age: {age.days} days > {self.max_age_days} days)")
                return False

            return True

        except Exception as e:
            warnings.warn(f"Error reading cache metadata: {e}")
            return False

    def save_model(
        self,
        cache_key: str,
        model: Any,
        metadata: Optional[Dict] = None
    ):
        """
        Save trained model to cache.

        ## What Gets Saved:

        1. **Model file** (.pkl):
           - Trained synthesizer object
           - Model weights
           - Internal state

        2. **Metadata file** (.meta.json):
           - Cache timestamp
           - Data shape
           - Training time
           - Model config
           - File size

        Args:
            cache_key: Cache key from generate_cache_key()
            model: Trained synthesizer model
            metadata: Additional metadata to store (optional)

        Example:
            >>> cache.save_model(
            ...     cache_key='abc123_def456_CTGAN',
            ...     model=trained_generator,
            ...     metadata={'training_time': 300, 'epochs': 100}
            ... )
        """
        if not self.enabled:
            return

        cache_path = self._get_cache_path(cache_key)
        meta_path = self._get_metadata_path(cache_key)

        try:
            # Save model
            with open(cache_path, 'wb') as f:
                pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)

            # Prepare metadata
            model_metadata = {
                'cache_key': cache_key,
                'cached_at': datetime.now().isoformat(),
                'file_size_mb': cache_path.stat().st_size / (1024 ** 2),
                'last_accessed': datetime.now().isoformat()
            }

            if metadata:
                model_metadata.update(metadata)

            # Save metadata
            with open(meta_path, 'w') as f:
                json.dump(model_metadata, f, indent=2)

            if self.verbose:
                print(f"  üíæ Cached model: {cache_key}")
                print(f"     Size: {model_metadata['file_size_mb']:.2f} MB")

            # Check and enforce cache size limit
            self._enforce_cache_limits()

        except Exception as e:
            warnings.warn(f"Failed to save model to cache: {e}")

    def load_model(self, cache_key: str) -> Optional[Any]:
        """
        Load trained model from cache.

        ## Loading Process:

        1. Check if cache exists and is valid
        2. Update "last accessed" timestamp (for LRU)
        3. Load and deserialize model
        4. Return model object

        Args:
            cache_key: Cache key from generate_cache_key()

        Returns:
            Loaded model object, or None if not cached

        Example:
            >>> model = cache.load_model('abc123_def456_CTGAN')
            >>> if model:
            ...     synthetic = model.sample(1000)
            ... else:
            ...     # Train new model
        """
        if not self.enabled or not self.has_cached_model(cache_key):
            return None

        cache_path = self._get_cache_path(cache_key)
        meta_path = self._get_metadata_path(cache_key)

        try:
            # Load model
            with open(cache_path, 'rb') as f:
                model = pickle.load(f)

            # Update last accessed time (for LRU)
            try:
                with open(meta_path, 'r') as f:
                    metadata = json.load(f)

                metadata['last_accessed'] = datetime.now().isoformat()

                with open(meta_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
            except Exception:
                pass  # Non-critical

            if self.verbose:
                print(f"  ‚ö° Loaded from cache: {cache_key}")

            return model

        except Exception as e:
            warnings.warn(f"Failed to load model from cache: {e}")
            return None

    def _enforce_cache_limits(self):
        """
        Enforce cache size and age limits.

        ## Cleanup Strategy:

        1. **Remove expired entries** (age > max_age_days)
        2. **Check total size**
        3. **If over limit**: Remove least recently used (LRU)

        This runs automatically after each save operation.
        """
        if not self.enabled:
            return

        # Get all cache entries
        cache_files = list(self.cache_dir.glob("*.pkl"))
        total_size_gb = sum(f.stat().st_size for f in cache_files) / (1024 ** 3)

        # Remove expired entries
        expired = []
        for pkl_file in cache_files:
            meta_file = pkl_file.with_suffix('.meta.json')

            if meta_file.exists():
                try:
                    with open(meta_file, 'r') as f:
                        metadata = json.load(f)

                    cached_time = datetime.fromisoformat(metadata['cached_at'])
                    age = datetime.now() - cached_time

                    if age > timedelta(days=self.max_age_days):
                        expired.append((pkl_file, meta_file))

                except Exception:
                    # Corrupted metadata, remove
                    expired.append((pkl_file, meta_file))

        # Delete expired
        for pkl_file, meta_file in expired:
            pkl_file.unlink(missing_ok=True)
            meta_file.unlink(missing_ok=True)

        if expired and self.verbose:
            print(f"  üóëÔ∏è  Removed {len(expired)} expired cache entries")

        # Recalculate size
        cache_files = list(self.cache_dir.glob("*.pkl"))
        total_size_gb = sum(f.stat().st_size for f in cache_files) / (1024 ** 3)

        # If still over limit, use LRU
        if total_size_gb > self.max_cache_size_gb:
            # Sort by last accessed (LRU)
            entries = []
            for pkl_file in cache_files:
                meta_file = pkl_file.with_suffix('.meta.json')

                if meta_file.exists():
                    try:
                        with open(meta_file, 'r') as f:
                            metadata = json.load(f)

                        last_accessed = datetime.fromisoformat(metadata['last_accessed'])
                        entries.append((last_accessed, pkl_file, meta_file))

                    except Exception:
                        pass

            # Sort by last accessed (oldest first)
            entries.sort(key=lambda x: x[0])

            # Remove until under limit
            for last_accessed, pkl_file, meta_file in entries:
                pkl_file.unlink(missing_ok=True)
                meta_file.unlink(missing_ok=True)

                # Recalculate
                cache_files = list(self.cache_dir.glob("*.pkl"))
                total_size_gb = sum(f.stat().st_size for f in cache_files) / (1024 ** 3)

                if total_size_gb <= self.max_cache_size_gb:
                    break

            if self.verbose:
                print(f"  üóëÔ∏è  LRU cleanup: now {total_size_gb:.2f} GB / {self.max_cache_size_gb} GB")

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics and usage information.

        Returns:
            Dict with:
            - total_entries: Number of cached models
            - total_size_gb: Total cache size in GB
            - oldest_entry: Date of oldest cache
            - newest_entry: Date of newest cache
            - entries: List of cache entries with details
        """
        if not self.enabled or not self.cache_dir.exists():
            return {
                'total_entries': 0,
                'total_size_gb': 0.0,
                'entries': []
            }

        cache_files = list(self.cache_dir.glob("*.pkl"))
        total_size = sum(f.stat().st_size for f in cache_files)

        entries = []
        oldest_time = None
        newest_time = None

        for pkl_file in cache_files:
            meta_file = pkl_file.with_suffix('.meta.json')

            if meta_file.exists():
                try:
                    with open(meta_file, 'r') as f:
                        metadata = json.load(f)

                    cached_time = datetime.fromisoformat(metadata['cached_at'])

                    if oldest_time is None or cached_time < oldest_time:
                        oldest_time = cached_time
                    if newest_time is None or cached_time > newest_time:
                        newest_time = cached_time

                    entries.append({
                        'cache_key': metadata.get('cache_key', pkl_file.stem),
                        'size_mb': metadata.get('file_size_mb', 0),
                        'cached_at': metadata.get('cached_at'),
                        'last_accessed': metadata.get('last_accessed'),
                        'age_days': (datetime.now() - cached_time).days
                    })

                except Exception:
                    pass

        return {
            'total_entries': len(cache_files),
            'total_size_gb': total_size / (1024 ** 3),
            'oldest_entry': oldest_time.isoformat() if oldest_time else None,
            'newest_entry': newest_time.isoformat() if newest_time else None,
            'entries': entries
        }

    def clear_cache(self, confirm: bool = False):
        """
        Clear all cached models.

        **Warning:** This is destructive and cannot be undone!

        Args:
            confirm: Must be True to actually clear (safety check)

        Example:
            >>> cache.clear_cache(confirm=True)  # Deletes all cache
        """
        if not confirm:
            print("‚ö†Ô∏è  Set confirm=True to actually clear cache")
            return

        if not self.cache_dir.exists():
            return

        # Remove all .pkl and .meta.json files
        for file in self.cache_dir.glob("*.pkl"):
            file.unlink()
        for file in self.cache_dir.glob("*.meta.json"):
            file.unlink()

        if self.verbose:
            print(f"  üóëÔ∏è  Cache cleared: {self.cache_dir}")


# Example usage and testing
if __name__ == "__main__":
    print("="*70)
    print("MODEL CACHE - DEMONSTRATION")
    print("="*70)

    # Initialize cache
    print("\n1. Initializing ModelCache...")
    cache = ModelCache(
        cache_dir="cache/models",
        max_age_days=30,
        max_cache_size_gb=5.0,
        verbose=True
    )

    # Create sample data
    print("\n" + "="*70)
    print("2. Creating sample dataset...")
    print("="*70)
    df = pd.DataFrame({
        'Age': np.random.randint(18, 80, 100),
        'BMI': np.random.normal(25, 5, 100),
        'Glucose': np.random.randint(70, 150, 100)
    })
    print(f"  Shape: {df.shape}")

    # Generate cache key
    print("\n" + "="*70)
    print("3. Generating cache key...")
    print("="*70)
    cache_key = cache.generate_cache_key(
        df=df,
        method='CTGAN',
        config={'epochs': 100, 'batch_size': 500}
    )
    print(f"  Cache key: {cache_key}")

    # Simulate saving model
    print("\n" + "="*70)
    print("4. Simulating model save...")
    print("="*70)

    # Create dummy model (in reality, this would be trained CTGAN)
    dummy_model = {
        'type': 'CTGAN',
        'trained': True,
        'data_shape': df.shape
    }

    cache.save_model(
        cache_key=cache_key,
        model=dummy_model,
        metadata={
            'data_shape': df.shape,
            'training_time_sec': 300,
            'method': 'CTGAN'
        }
    )

    # Check if cached
    print("\n" + "="*70)
    print("5. Checking cache...")
    print("="*70)
    is_cached = cache.has_cached_model(cache_key)
    print(f"  Model cached: {is_cached}")

    # Load from cache
    print("\n" + "="*70)
    print("6. Loading from cache...")
    print("="*70)
    loaded_model = cache.load_model(cache_key)
    if loaded_model:
        print(f"  ‚úì Model loaded successfully!")
        print(f"  Model type: {loaded_model['type']}")
        print(f"  Data shape: {loaded_model['data_shape']}")

    # Get cache statistics
    print("\n" + "="*70)
    print("7. Cache Statistics")
    print("="*70)
    stats = cache.get_cache_stats()
    print(f"  Total entries: {stats['total_entries']}")
    print(f"  Total size: {stats['total_size_gb']:.3f} GB")
    print(f"  Newest entry: {stats['newest_entry']}")

    print("\n" + "="*70)
    print("‚úì Demonstration Complete!")
    print("="*70)
